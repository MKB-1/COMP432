{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Imports...\n",
    "We chose to go with urllib instead of wget because we want to specify file save location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/leboi/git/school/comp432/ML-project\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from env import DIR\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import urllib3\n",
    "\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fetching the data with urllib, converting it from text\n",
    "We use a list of dataset configs to help us save the data and preprocess it.\n",
    "The config is different for each file extension.\n",
    "config = [\n",
    "    {\n",
    "    'name': str,\n",
    "        'url': str,\n",
    "        'file_extension': 'xls | arr ',\n",
    "        'id': int,\n",
    "        &unique properties\n",
    "    }\n",
    "]\n",
    "\n",
    "unique properties:\n",
    "'xls' -> 'sheet_names': str[]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "DATASET_CONFIGS = [\n",
    "    {\n",
    "        'name': 'credit_card_defaults',\n",
    "        'url': 'https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls',\n",
    "        'sheet_names': ['Data'],\n",
    "        'file_extension': 'xls',\n",
    "        'id': 1\n",
    "    }\n",
    "]\n",
    "\n",
    "BASE_PATH = 'data'\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    Loads various data formats and converts them to csv by using a config dictionary.\n",
    "    config = {'file_extension': ['download_url']}\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.http = urllib3.PoolManager()\n",
    "        self.base_path = BASE_PATH\n",
    "\n",
    "        if not os.path.exists(self.base_path):\n",
    "            os.mkdir(self.base_path)\n",
    "\n",
    "    def fetch_data(self, url, dataset_name, file_ext):\n",
    "        dataset_dir = f'{self.base_path}/{dataset_name}'\n",
    "        if not os.path.exists(dataset_dir):\n",
    "            os.mkdir(dataset_dir)\n",
    "        with open(f'{dataset_dir}/raw_data.{file_ext}', 'wb') as f:\n",
    "            res = self.http.request('GET', url, preload_content=False)\n",
    "            shutil.copyfileobj(res, f)\n",
    "\n",
    "    \"\"\"\n",
    "    Converts a single sheet from an excel file to a csv file, by loading it as a dataframe with pandas, and writing a csv file with pd.to_csv()\n",
    "    \"\"\"\n",
    "    def xls_to_csv(self, dataset_name, sheet_name):\n",
    "        xl_path = f'{self.base_path}/{dataset_name}/raw_data.xls'\n",
    "        csv_path = f'{self.base_path}/{dataset_name}/raw_data.csv'\n",
    "        xl =  pd.ExcelFile(xl_path)\n",
    "        df = xl.parse(sheetname=sheet_name, index_col=None, na_values=[''])\n",
    "        df.to_csv(csv_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we fetch the data from the servers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "loader = DataLoader(DATASET_CONFIGS)\n",
    "name = loader.config[0]['name']\n",
    "url = loader.config[0]['url']\n",
    "sheet_name = loader.config[0]['sheet_names'][0]\n",
    "file_ext = loader.config[0]['file_extension']\n",
    "loader.fetch_data(url, name, file_ext)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wait until the files have been created before running the next block.\n",
    "Here, we preprocess the data and convert it from its original format into a csv that is easy to load into pandas."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'xlrd'. Install xlrd >= 1.0.0 for Excel support Use pip or conda to install xlrd.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/f1/2zh1m6x57zl_44cx3zy96hfh0000gn/T/ipykernel_99934/3565964104.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mloader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mxls_to_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msheet_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/f1/2zh1m6x57zl_44cx3zy96hfh0000gn/T/ipykernel_99934/2008670231.py\u001B[0m in \u001B[0;36mxls_to_csv\u001B[0;34m(self, dataset_name, sheet_name)\u001B[0m\n\u001B[1;32m     38\u001B[0m         \u001B[0mxl_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34mf'{self.base_path}/{dataset_name}/raw_data.xls'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     39\u001B[0m         \u001B[0mcsv_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34mf'{self.base_path}/{dataset_name}/raw_data.csv'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 40\u001B[0;31m         \u001B[0mxl\u001B[0m \u001B[0;34m=\u001B[0m  \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mExcelFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mxl_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     41\u001B[0m         \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mxl\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparse\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msheetname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msheet_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex_col\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mna_values\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m''\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m         \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcsv_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/comp-432-ml-project/lib/python3.8/site-packages/pandas/io/excel/_base.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, path_or_buffer, engine, storage_options)\u001B[0m\n\u001B[1;32m   1231\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstorage_options\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstorage_options\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1232\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1233\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reader\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engines\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_io\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstorage_options\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstorage_options\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1234\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1235\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__fspath__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/comp-432-ml-project/lib/python3.8/site-packages/pandas/io/excel/_xlrd.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, filepath_or_buffer, storage_options)\u001B[0m\n\u001B[1;32m     22\u001B[0m         \"\"\"\n\u001B[1;32m     23\u001B[0m         \u001B[0merr_msg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 24\u001B[0;31m         \u001B[0mimport_optional_dependency\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"xlrd\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mextra\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0merr_msg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     25\u001B[0m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstorage_options\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstorage_options\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/comp-432-ml-project/lib/python3.8/site-packages/pandas/compat/_optional.py\u001B[0m in \u001B[0;36mimport_optional_dependency\u001B[0;34m(name, extra, errors, min_version)\u001B[0m\n\u001B[1;32m    116\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mImportError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    117\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0merrors\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"raise\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 118\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mImportError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    119\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    120\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mImportError\u001B[0m: Missing optional dependency 'xlrd'. Install xlrd >= 1.0.0 for Excel support Use pip or conda to install xlrd."
     ]
    }
   ],
   "source": [
    "loader.xls_to_csv(name, sheet_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since each data set is unique, we must write code which is unique to that data set in order to clean it up.\n",
    "We define a class with helper methods which will allow us to extract the required information from a dataset, when provided with the dataset's id\n",
    "We would like to automate some of the process', therefore we must take care to preprocess the data from different datasets in a similar fashion."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \"\"\"\n",
    "    Removes the first column and the second row of the credit card defaults data set.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.base_path = BASE_PATH\n",
    "\n",
    "    def dataset_1(self):\n",
    "        raw_path, clean_path = self._generate_paths(1)\n",
    "        df = pd.read_csv(raw_path)\n",
    "\n",
    "        # Unique clean up\n",
    "        df.drop(columns=df.columns[0:2], inplace=True)\n",
    "        df.drop(index=df.index[0], axis=0, inplace=True)\n",
    "        df.to_csv(clean_path, header=True, index=False)\n",
    "\n",
    "    def _filter(self, dataset_id):\n",
    "        return [d for d in self.config if d['id'] == dataset_id][0]\n",
    "\n",
    "    def _generate_paths(self, dataset_id):\n",
    "        dataset = self._filter(dataset_id)\n",
    "        ds_name = dataset['name']\n",
    "        raw_path = f'{self.base_path}/{ds_name}/raw_data.csv'\n",
    "        clean_path = f'{self.base_path}/{ds_name}/clean_data.csv'\n",
    "        return raw_path, clean_path\n",
    "\n",
    "\n",
    "cleaner = DataPreprocessor(DATASET_CONFIGS)\n",
    "cleaner.dataset_1()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We must now do some grunt work in order to automate further preprocessing.\n",
    "We must create a feature map for our clean_data file. The feature map is a csv file of size (\n",
    "It must then be reloaded with the appropriate types, and with its corresponding feature map.\n",
    "The feature map must be of size (5, N+1). Where N is the number of features.\n",
    "The $\\textbf{first row}$ is reserved for feature names (e.g. X1, X2, ... )\n",
    "The $\\textbf{first column}$ is reserved for indexing. It MUST contain 'Feature', 'Name', 'Description, 'Unit', and 'Data type'.\n",
    "Data type must be 'Numerical continuous', 'Numerical discrete', 'Categorical nominal', or 'Categorical ordinal'\n",
    "\n",
    "Since each dataset presents the meaning of its variables in a unique way, we cannot automate this part. However, this is crucial in order to automate basic data analysis, feature preprocessing, and graph generation\n",
    "\n",
    "We create a list of dicts to organize the information for all datasets.\n",
    "Since we cannot provide arguments to defaultdict, we create a helper function to generate a dict for us.\n",
    "The helper function uses some global variables, but we'll pass them as arguments to keep the function pure.\n",
    "\n",
    "We decide to index dataframes by names instead of by id, because when testing models it is helpful to know exactly which dataset you're using."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def df_dict_generator(config, base_path):\n",
    "    res = {}\n",
    "    for dataset in config:\n",
    "        ds_name = dataset['name']  # TODO: if dict value is used more than once in string, use variable do avoid double quotation marks for dict key within f-string\n",
    "        res[name] = {\n",
    "            'data': pd.read_csv(f'{BASE_PATH}/{ds_name}/clean_data.csv', header=0),\n",
    "            'map': pd.read_csv(f'{BASE_PATH}/{ds_name}/map.csv', header=0, index_col=0).to_dict()\n",
    "        }\n",
    "\n",
    "    return res\n",
    "\n",
    "DATAFRAMES = df_dict_generator(DATASET_CONFIGS, BASE_PATH)\n",
    "\n",
    "# manual testing\n",
    "test_dfs_dict = DATAFRAMES['credit_card_defaults']\n",
    "test_dfs_dict['map']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Automating data analysis must be done in two steps:\n",
    "1. An overview to decide what to do with null data / non valid data, and other preprocessing. To do so, it would help to know how the null values correlate in the dataframe. (i.e. does Xi = NULL indicate Xk = NULL?) The data must be cleaned up manually before moving to step 2.\n",
    "2. Graphs will be generated for each feature, according to their data type. I suspect these graphs will not be optimal and will require case by case tuning, but it should at least give us a good idea on which graphs are interesting.\n",
    "\n",
    "The Analyzer class helps us with three methods:\n",
    "1. overview(): analyzes each column based on the data type it contains. Provides a high level statistical analysis, and some graphs. This allows us to quickly have an idea of how the data looks, and allows us to manually ignore or drop nulls. Furthermore, it allows us to know what type of validation is required (to null or drop invalid data).\n",
    "2. visualize_2d_feature_relationships(x, y_list): generates a graph for every pair x, yi. The type of graph is determined based on the data type of x and yi.\n",
    "3. train_models(): applies transformations on the dataset (scaling, one hot encoding), splits the data into training/testing sets, and trains a variety of models (with randomized hyper-parameters defined in the constructor). It prints out a classification report.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Analyzer:\n",
    "    def __init__(self, df: pd.DataFrame, f_desc: dict, model_hyperparameters):\n",
    "        self.rstate = 1\n",
    "        self.df = df\n",
    "        self.x = df.drop(columns=['Y'])\n",
    "        self.y = df['Y']\n",
    "        self.feature_desc = f_desc\n",
    "        self.data_type_cols = defaultdict(lambda : [])\n",
    "        self.model_hyperparameters = model_hyperparameters\n",
    "        for f, info in self.feature_desc.items():\n",
    "            if f == 'Y':\n",
    "                continue\n",
    "            if info['Data type'] == 'Categorical nominal' or info['Data type'] == 'Categorical ordinal' or info['Data type'] == 'Numerical continuous' or info['Data type'] == 'Numerical discrete':\n",
    "                self.data_type_cols[info['Data type']].append(f)\n",
    "            else:\n",
    "                raise Exception(f'Data type \"{info[\"Data type\"]}\" is not supported')\n",
    "\n",
    "    def overview(self):\n",
    "        for feature in self.df:\n",
    "            s = self.df[feature]\n",
    "            if feature in self.feature_desc:\n",
    "                description = self.feature_desc[feature]['Description']\n",
    "                name = self.feature_desc[feature]['Name']\n",
    "                unit = self.feature_desc[feature]['Unit']\n",
    "                data_type = self.feature_desc[feature]['Data type']\n",
    "\n",
    "                print(f'{feature}: {name}')\n",
    "                print(description)\n",
    "                print(f'units: {unit}')\n",
    "                print(f'Data type: {data_type}', '\\n')\n",
    "                # TODO: Make validator method. Will require new row in feature map\n",
    "                print(f'Null/invalid values: {s.isna().sum()}')\n",
    "\n",
    "                if data_type == 'Categorical nominal':\n",
    "                    print(s.value_counts())\n",
    "\n",
    "                elif data_type == 'Categorical ordinal':\n",
    "                    print(s.value_counts())\n",
    "\n",
    "                elif data_type == 'Numerical continuous':\n",
    "                    print(s.describe(), '\\n')\n",
    "\n",
    "                elif data_type == 'Numerical discrete':\n",
    "                    print(s.describe(), '\\n')\n",
    "                else:\n",
    "                    raise Exception(f'Data type \"{data_type}\" is not supported')\n",
    "\n",
    "                plt.figure()\n",
    "                s.plot.kde(title=f'{name} density distribution')\n",
    "                print('\\n\\n')\n",
    "\n",
    "    def train_models(self):\n",
    "        X_train, X_test, y_train, y_test = self._generate_train_test_datasets()\n",
    "        for m in self.model_hyperparameters:\n",
    "            clf = RandomizedSearchCV(m['model'], m['hyper_parameters'], random_state=self.rstate)\n",
    "            clf.fit(X_train, y_train)\n",
    "            log_reg_prediction = clf.predict(X_test)\n",
    "            print('\\n',clf.best_params_)\n",
    "            print(classification_report(y_test, log_reg_prediction))\n",
    "\n",
    "    def visualize_2d_feature_relationships(self, x_feature, y_features):\n",
    "        for y_feature in y_features:\n",
    "            fig, ax = plt.subplots(figsize=(10,8))\n",
    "            plt.suptitle('')\n",
    "            self.df.boxplot(column=[y_feature], by=x_feature, ax=ax)\n",
    "\n",
    "    # TODO: does pipeline shit like scale numerical features\n",
    "    def _generate_ct_transformers(self):\n",
    "        transformers = []\n",
    "        for dt, cols in self.data_type_cols.items():\n",
    "            if dt == 'Categorical nominal':\n",
    "                transformers.append(('one_hot_encoding', OneHotEncoder(handle_unknown='ignore', sparse=False), cols))\n",
    "            elif dt == 'Categorical ordinal':\n",
    "                continue\n",
    "            elif dt == 'Numerical continuous':\n",
    "                continue\n",
    "            elif dt == 'Numerical discrete':\n",
    "                transformers.append(('standard_scaler', StandardScaler(), cols))\n",
    "\n",
    "        return transformers\n",
    "\n",
    "    def _generate_transformed_array(self):\n",
    "        ct = ColumnTransformer(transformers=self._generate_ct_transformers(), remainder='passthrough')\n",
    "        arr = ct.fit_transform(self.x)\n",
    "        return arr\n",
    "\n",
    "    def _generate_train_test_datasets(self):\n",
    "        X = self._generate_transformed_array()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, self.y.to_numpy(), test_size=0.33, random_state=self.rstate)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def _generate_logistic_regression_model(self, hyper_parameters: dict):\n",
    "        # The solving algorithm depends on the penalty chosen. Some solvers cannot solve all penalties.\n",
    "        # 'saga' solving REQUIRES feature scaling for fast convergence.\n",
    "        solver_combinations = [('lbfgs', ['l2', 'none']),  # default\n",
    "                               ('saga', ['elasticnet', 'l1', 'l2', 'none']),\n",
    "                               ('liblinear', ['l1', 'l2'])]\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Example use"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hparams = [{\n",
    "    'model': LogisticRegression(),\n",
    "    'hyper_parameters': {\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'C': np.linspace(0.6, 1.4, num=8)\n",
    "    }\n",
    "}\n",
    "]\n",
    "\n",
    "analyzer = Analyzer(test_dfs_dict['data'], test_dfs_dict['map'], hparams)\n",
    "analyzer.overview()\n",
    "analyzer.visualize_2d_feature_relationships('X2', ['X9'])\n",
    "analyzer.train_models()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}