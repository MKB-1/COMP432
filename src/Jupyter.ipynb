{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Imports...\n",
    "We chose to go with urllib instead of wget because we want to specify file save location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from env import DIR\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import urllib3\n",
    "\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Fetching the data with urllib, converting it from text\n",
    "We use a list of dataset configs to help us save the data and preprocess it.\n",
    "The config is different for each file extension.\n",
    "config = [\n",
    "    {\n",
    "    'name': str,\n",
    "        'url': str,\n",
    "        'file_extension': 'xls | arr ',\n",
    "        'id': int,\n",
    "        &unique properties\n",
    "    }\n",
    "]\n",
    "\n",
    "unique properties:\n",
    "'xls' -> 'sheet_names': str[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_CONFIGS = [\n",
    "    {\n",
    "        'name': 'credit_card_defaults',\n",
    "        'url': 'https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls',\n",
    "        'sheet_names': ['Data'],\n",
    "        'file_extension': 'xls',\n",
    "        'id': 1\n",
    "    }\n",
    "]\n",
    "\n",
    "BASE_PATH = DIR.res_path(['data'])\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    Loads various data formats and converts them to csv by using a config dictionary.\n",
    "    config = {'file_extension': ['download_url']}\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.http = urllib3.PoolManager()\n",
    "        self.base_path = BASE_PATH\n",
    "\n",
    "        if not os.path.exists(self.base_path):\n",
    "            os.mkdir(self.base_path)\n",
    "\n",
    "    def fetch_data(self, url, dataset_name, file_ext):\n",
    "        dataset_dir = f'{self.base_path}/{dataset_name}'\n",
    "        if not os.path.exists(dataset_dir):\n",
    "            os.mkdir(dataset_dir)\n",
    "        with open(f'{dataset_dir}/raw_data.{file_ext}', 'wb') as f:\n",
    "            res = self.http.request('GET', url, preload_content=False)\n",
    "            shutil.copyfileobj(res, f)\n",
    "\n",
    "    \"\"\"\n",
    "    Converts a single sheet from an excel file to a csv file, by loading it as a dataframe with pandas, and writing a csv file with pd.to_csv()\n",
    "    \"\"\"\n",
    "    def xls_to_csv(self, dataset_name, sheet_name):\n",
    "        xl_path = f'{self.base_path}/{dataset_name}/raw_data.xls'\n",
    "        csv_path = f'{self.base_path}/{dataset_name}/raw_data.csv'\n",
    "        xl =  pd.ExcelFile(xl_path)\n",
    "        df = xl.parse(sheetname=sheet_name, index_col=None, na_values=[''])\n",
    "        df.to_csv(csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we fetch the data from the servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loader = DataLoader(DATASET_CONFIGS)\n",
    "name = loader.config[0]['name']\n",
    "url = loader.config[0]['url']\n",
    "sheet_name = loader.config[0]['sheet_names'][0]\n",
    "file_ext = loader.config[0]['file_extension']\n",
    "loader.fetch_data(url, name, file_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Wait until the files have been created before running the next block.\n",
    "Here, we preprocess the data and convert it from its original format into a csv that is easy to load into pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loader.xls_to_csv(name, sheet_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since each data set is unique, we must write code which is unique to that data set in order to clean it up.\n",
    "We define a class with helper methods which will allow us to extract the required information from a dataset, when provided with the dataset's id\n",
    "We would like to automate some of the process', therefore we must take care to preprocess the data from different datasets in a similar fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \"\"\"\n",
    "    Removes the first column and the second row of the credit card defaults data set.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.base_path = BASE_PATH\n",
    "\n",
    "    def dataset_1(self):\n",
    "        raw_path, clean_path = self._generate_paths(1)\n",
    "        df = pd.read_csv(raw_path)\n",
    "\n",
    "        # Unique clean up\n",
    "        df.drop(columns=df.columns[0:2], inplace=True)\n",
    "        df.drop(index=df.index[0], axis=0, inplace=True)\n",
    "        df.to_csv(clean_path, header=True, index=False)\n",
    "\n",
    "    def _filter(self, dataset_id):\n",
    "        return [d for d in self.config if d['id'] == dataset_id][0]\n",
    "\n",
    "    def _generate_paths(self, dataset_id):\n",
    "        dataset = self._filter(dataset_id)\n",
    "        ds_name = dataset['name']\n",
    "        raw_path = f'{self.base_path}/{ds_name}/raw_data.csv'\n",
    "        clean_path = f'{self.base_path}/{ds_name}/clean_data.csv'\n",
    "        return raw_path, clean_path\n",
    "\n",
    "\n",
    "cleaner = DataPreprocessor(DATASET_CONFIGS)\n",
    "cleaner.dataset_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We must now do some grunt work in order to automate further preprocessing.\n",
    "We must create a feature map for our clean_data file. The feature map is a csv file of size (\n",
    "It must then be reloaded with the appropriate types, and with its corresponding feature map.\n",
    "The feature map must be of size (5, N+1). Where N is the number of features.\n",
    "The $\\textbf{first row}$ is reserved for feature names (e.g. X1, X2, ... )\n",
    "The $\\textbf{first column}$ is reserved for indexing. It MUST contain 'Feature', 'Name', 'Description, 'Unit', and 'Data type'.\n",
    "Data type must be 'Numerical continuous', 'Numerical discrete', 'Categorical nominal', or 'Categorical ordinal'\n",
    "\n",
    "Since each dataset presents the meaning of its variables in a unique way, we cannot automate this part. However, this is crucial in order to automate basic data analysis, feature preprocessing, and graph generation\n",
    "\n",
    "We create a list of dicts to organize the information for all datasets.\n",
    "Since we cannot provide arguments to defaultdict, we create a helper function to generate a dict for us.\n",
    "The helper function uses some global variables, but we'll pass them as arguments to keep the function pure.\n",
    "\n",
    "We decide to index dataframes by names instead of by id, because when testing models it is helpful to know exactly which dataset you're using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/credit_card_defaults/map.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/f1/2zh1m6x57zl_44cx3zy96hfh0000gn/T/ipykernel_8275/2329999256.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mres\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m \u001B[0mDATAFRAMES\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf_dict_generator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mDATASET_CONFIGS\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mBASE_PATH\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;31m# manual testing\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/f1/2zh1m6x57zl_44cx3zy96hfh0000gn/T/ipykernel_8275/2329999256.py\u001B[0m in \u001B[0;36mdf_dict_generator\u001B[0;34m(config, base_path)\u001B[0m\n\u001B[1;32m      5\u001B[0m         res[name] = {\n\u001B[1;32m      6\u001B[0m             \u001B[0;34m'data'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'{BASE_PATH}/{ds_name}/clean_data.csv'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mheader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m             \u001B[0;34m'map'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'{BASE_PATH}/{ds_name}/map.csv'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mheader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex_col\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m         }\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/comp-432-ml-project/lib/python3.8/site-packages/pandas/util/_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    310\u001B[0m                 )\n\u001B[0;32m--> 311\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    312\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    313\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/comp-432-ml-project/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[1;32m    584\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    585\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 586\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    587\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    588\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/comp-432-ml-project/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    480\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    481\u001B[0m     \u001B[0;31m# Create the parser.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 482\u001B[0;31m     \u001B[0mparser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    483\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    484\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/comp-432-ml-project/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m    809\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    810\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 811\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    812\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    813\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/comp-432-ml-project/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[0;34m(self, engine)\u001B[0m\n\u001B[1;32m   1038\u001B[0m             )\n\u001B[1;32m   1039\u001B[0m         \u001B[0;31m# error: Too many arguments for \"ParserBase\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1040\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mmapping\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[call-arg]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1041\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1042\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_failover_to_python\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/comp-432-ml-project/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, src, **kwds)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m         \u001B[0;31m# open handles\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_open_handles\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     52\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhandles\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/comp-432-ml-project/lib/python3.8/site-packages/pandas/io/parsers/base_parser.py\u001B[0m in \u001B[0;36m_open_handles\u001B[0;34m(self, src, kwds)\u001B[0m\n\u001B[1;32m    220\u001B[0m         \u001B[0mLet\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mreaders\u001B[0m \u001B[0mopen\u001B[0m \u001B[0mIOHandles\u001B[0m \u001B[0mafter\u001B[0m \u001B[0mthey\u001B[0m \u001B[0mare\u001B[0m \u001B[0mdone\u001B[0m \u001B[0;32mwith\u001B[0m \u001B[0mtheir\u001B[0m \u001B[0mpotential\u001B[0m \u001B[0mraises\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    221\u001B[0m         \"\"\"\n\u001B[0;32m--> 222\u001B[0;31m         self.handles = get_handle(\n\u001B[0m\u001B[1;32m    223\u001B[0m             \u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    224\u001B[0m             \u001B[0;34m\"r\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/comp-432-ml-project/lib/python3.8/site-packages/pandas/io/common.py\u001B[0m in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    700\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencoding\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;34m\"b\"\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    701\u001B[0m             \u001B[0;31m# Encoding\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 702\u001B[0;31m             handle = open(\n\u001B[0m\u001B[1;32m    703\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    704\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data/credit_card_defaults/map.csv'"
     ]
    }
   ],
   "source": [
    "def df_dict_generator(config, base_path):\n",
    "    res = {}\n",
    "    for dataset in config:\n",
    "        ds_name = dataset['name']  # TODO: if dict value is used more than once in string, use variable do avoid double quotation marks for dict key within f-string\n",
    "        res[name] = {\n",
    "            'data': pd.read_csv(f'{BASE_PATH}/{ds_name}/clean_data.csv', header=0),\n",
    "            'map': pd.read_csv(f'{BASE_PATH}/{ds_name}/map.csv', header=0, index_col=0).to_dict()\n",
    "        }\n",
    "\n",
    "    return res\n",
    "\n",
    "DATAFRAMES = df_dict_generator(DATASET_CONFIGS, BASE_PATH)\n",
    "\n",
    "# manual testing\n",
    "test_dfs_dict = DATAFRAMES['credit_card_defaults']\n",
    "test_dfs_dict['map']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Automating data analysis must be done in two steps:\n",
    "1. An overview to decide what to do with null data / non valid data, and other preprocessing. To do so, it would help to know how the null values correlate in the dataframe. (i.e. does Xi = NULL indicate Xk = NULL?) The data must be cleaned up manually before moving to step 2.\n",
    "2. Graphs will be generated for each feature, according to their data type. I suspect these graphs will not be optimal and will require case by case tuning, but it should at least give us a good idea on which graphs are interesting.\n",
    "\n",
    "The Analyzer class helps us with three methods:\n",
    "1. overview(): analyzes each column based on the data type it contains. Provides a high level statistical analysis, and some graphs. This allows us to quickly have an idea of how the data looks, and allows us to manually ignore or drop nulls. Furthermore, it allows us to know what type of validation is required (to null or drop invalid data).\n",
    "2. visualize_2d_feature_relationships(x, y_list): generates a graph for every pair x, yi. The type of graph is determined based on the data type of x and yi.\n",
    "3. train_models(): applies transformations on the dataset (scaling, one hot encoding), splits the data into training/testing sets, and trains a variety of models (with randomized hyper-parameters defined in the constructor). It prints out a classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Analyzer:\n",
    "    def __init__(self, df: pd.DataFrame, f_desc: dict, model_hyperparameters):\n",
    "        self.rstate = 1\n",
    "        self.df = df\n",
    "        self.x = df.drop(columns=['Y'])\n",
    "        self.y = df['Y']\n",
    "        self.feature_desc = f_desc\n",
    "        self.data_type_cols = defaultdict(lambda : [])\n",
    "        self.model_hyperparameters = model_hyperparameters\n",
    "        for f, info in self.feature_desc.items():\n",
    "            if f == 'Y':\n",
    "                continue\n",
    "            if info['Data type'] == 'Categorical nominal' or info['Data type'] == 'Categorical ordinal' or info['Data type'] == 'Numerical continuous' or info['Data type'] == 'Numerical discrete':\n",
    "                self.data_type_cols[info['Data type']].append(f)\n",
    "            else:\n",
    "                raise Exception(f'Data type \"{info[\"Data type\"]}\" is not supported')\n",
    "\n",
    "    def overview(self):\n",
    "        for feature in self.df:\n",
    "            s = self.df[feature]\n",
    "            if feature in self.feature_desc:\n",
    "                description = self.feature_desc[feature]['Description']\n",
    "                name = self.feature_desc[feature]['Name']\n",
    "                unit = self.feature_desc[feature]['Unit']\n",
    "                data_type = self.feature_desc[feature]['Data type']\n",
    "\n",
    "                print(f'{feature}: {name}')\n",
    "                print(description)\n",
    "                print(f'units: {unit}')\n",
    "                print(f'Data type: {data_type}', '\\n')\n",
    "                # TODO: Make validator method. Will require new row in feature map\n",
    "                print(f'Null/invalid values: {s.isna().sum()}')\n",
    "\n",
    "                if data_type == 'Categorical nominal':\n",
    "                    print(s.value_counts())\n",
    "\n",
    "                elif data_type == 'Categorical ordinal':\n",
    "                    print(s.value_counts())\n",
    "\n",
    "                elif data_type == 'Numerical continuous':\n",
    "                    print(s.describe(), '\\n')\n",
    "\n",
    "                elif data_type == 'Numerical discrete':\n",
    "                    print(s.describe(), '\\n')\n",
    "                else:\n",
    "                    raise Exception(f'Data type \"{data_type}\" is not supported')\n",
    "\n",
    "                plt.figure()\n",
    "                s.plot.kde(title=f'{name} density distribution')\n",
    "                print('\\n\\n')\n",
    "\n",
    "    def train_models(self):\n",
    "        X_train, X_test, y_train, y_test = self._generate_train_test_datasets()\n",
    "        for m in self.model_hyperparameters:\n",
    "            clf = RandomizedSearchCV(m['model'], m['hyper_parameters'], random_state=self.rstate)\n",
    "            clf.fit(X_train, y_train)\n",
    "            log_reg_prediction = clf.predict(X_test)\n",
    "            print('\\n',clf.best_params_)\n",
    "            print(classification_report(y_test, log_reg_prediction))\n",
    "\n",
    "    def visualize_2d_feature_relationships(self, x_feature, y_features):\n",
    "        for y_feature in y_features:\n",
    "            fig, ax = plt.subplots(figsize=(10,8))\n",
    "            plt.suptitle('')\n",
    "            self.df.boxplot(column=[y_feature], by=x_feature, ax=ax)\n",
    "\n",
    "    # TODO: does pipeline shit like scale numerical features\n",
    "    def _generate_ct_transformers(self):\n",
    "        transformers = []\n",
    "        for dt, cols in self.data_type_cols.items():\n",
    "            if dt == 'Categorical nominal':\n",
    "                transformers.append(('one_hot_encoding', OneHotEncoder(handle_unknown='ignore', sparse=False), cols))\n",
    "            elif dt == 'Categorical ordinal':\n",
    "                continue\n",
    "            elif dt == 'Numerical continuous':\n",
    "                continue\n",
    "            elif dt == 'Numerical discrete':\n",
    "                transformers.append(('standard_scaler', StandardScaler(), cols))\n",
    "\n",
    "        return transformers\n",
    "\n",
    "    def _generate_transformed_array(self):\n",
    "        ct = ColumnTransformer(transformers=self._generate_ct_transformers(), remainder='passthrough')\n",
    "        arr = ct.fit_transform(self.x)\n",
    "        return arr\n",
    "\n",
    "    def _generate_train_test_datasets(self):\n",
    "        X = self._generate_transformed_array()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, self.y.to_numpy(), test_size=0.33, random_state=self.rstate)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def _generate_logistic_regression_model(self, hyper_parameters: dict):\n",
    "        # The solving algorithm depends on the penalty chosen. Some solvers cannot solve all penalties.\n",
    "        # 'saga' solving REQUIRES feature scaling for fast convergence.\n",
    "        solver_combinations = [('lbfgs', ['l2', 'none']),  # default\n",
    "                               ('saga', ['elasticnet', 'l1', 'l2', 'none']),\n",
    "                               ('liblinear', ['l1', 'l2'])]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Example use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hparams = [{\n",
    "    'model': LogisticRegression(),\n",
    "    'hyper_parameters': {\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'C': np.linspace(0.6, 1.4, num=8)\n",
    "    }\n",
    "}\n",
    "]\n",
    "\n",
    "analyzer = Analyzer(test_dfs_dict['data'], test_dfs_dict['map'], hparams)\n",
    "analyzer.overview()\n",
    "analyzer.visualize_2d_feature_relationships('X2', ['X9'])\n",
    "analyzer.train_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}